# Auto-generated by Manus: Chaos Triage Arena scaffold

import os
import requests
import json
from typing import Dict, List, Any, Optional


class LLMAdapter:
    """Adapter for LM Studio integration"""
    
    def __init__(self, base_url: str = "http://localhost:1234"):
        self.base_url = base_url
        self.enabled = os.getenv("USE_LLM", "false").lower() == "true"
    
    def is_available(self) -> bool:
        """Check if LLM service is available"""
        if not self.enabled:
            return False
        
        try:
            response = requests.get(f"{self.base_url}/v1/models", timeout=5)
            return response.status_code == 200
        except Exception:
            return False
    
    def generate_plan(self, incident: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:
        """Generate a recovery plan using LLM"""
        if not self.is_available():
            return None
        
        # Construct prompt for plan generation
        prompt = self._build_plan_prompt(incident)
        
        try:
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                json={
                    "model": "local-model",  # LM Studio uses this as default
                    "messages": [
                        {
                            "role": "system",
                            "content": "You are an expert system administrator creating incident response plans. Generate structured recovery plans as JSON."
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    "temperature": 0.3,
                    "max_tokens": 1000
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                # Try to parse the JSON response
                try:
                    plan_data = json.loads(content)
                    return self._validate_and_format_plan(plan_data, incident)
                except json.JSONDecodeError:
                    # If not valid JSON, fall back to stub planner
                    return None
            
        except Exception as e:
            print(f"[LLM_ADAPTER] Error calling LLM: {e}")
        
        return None
    
    def _build_plan_prompt(self, incident: Dict[str, Any]) -> str:
        """Build prompt for LLM plan generation"""
        service_name = incident.get("service", "unknown")
        status = incident.get("status", {})
        metrics = incident.get("metrics", {})
        
        prompt = f"""
Incident Report:
- Service: {service_name}
- Running: {status.get('running', False)}
- Healthy: {status.get('healthy', False)}
- CPU Usage: {metrics.get('cpu_percent', 0)}%
- Memory Usage: {metrics.get('memory_mb', 0)}MB
- Failed: {metrics.get('is_failed', False)}
- Failure Type: {metrics.get('failure_type', 'none')}

Available Agents:
- LogAgent: Analyzes service logs and error patterns
- MetricAgent: Collects and analyzes performance metrics
- ProbeAgent: Tests service connectivity and dependencies
- ActionAgent: Executes recovery actions (restart, recover, config changes)

Generate a recovery plan as JSON with this structure:
{{
  "steps": [
    {{
      "step": "step_name",
      "agent": "AgentName", 
      "dependencies": ["previous_step_names"],
      "description": "What this step does"
    }}
  ]
}}

Focus on:
1. Diagnosis before action
2. Checking dependencies
3. Gradual recovery approach
4. Monitoring after changes
"""
        return prompt
    
    def _validate_and_format_plan(self, plan_data: Dict[str, Any], incident: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Validate and format the LLM-generated plan"""
        if "steps" not in plan_data:
            return None
        
        valid_agents = ["LogAgent", "MetricAgent", "ProbeAgent", "ActionAgent"]
        formatted_plan = []
        
        for i, step in enumerate(plan_data["steps"]):
            if not isinstance(step, dict):
                continue
            
            step_name = step.get("step", f"step_{i}")
            agent = step.get("agent", "ActionAgent")
            dependencies = step.get("dependencies", [])
            description = step.get("description", "")
            
            # Validate agent
            if agent not in valid_agents:
                agent = "ActionAgent"  # Default fallback
            
            # Ensure dependencies are valid step names
            valid_dependencies = []
            for dep in dependencies:
                if any(s.get("step") == dep for s in formatted_plan):
                    valid_dependencies.append(dep)
            
            formatted_step = {
                "id": f"{incident.get('service', 'unknown')}_{step_name}_{int(time.time())}_{i}",
                "step": step_name,
                "agent": agent,
                "dependencies": valid_dependencies,
                "description": description,
                "incident_type": "llm_generated",
                "target_service": incident.get("service"),
                "status": "pending",
                "created_at": time.time()
            }
            
            formatted_plan.append(formatted_step)
        
        return formatted_plan if formatted_plan else None


def main():
    # Test the LLM adapter
    adapter = LLMAdapter()
    
    print(f"LLM enabled: {adapter.enabled}")
    print(f"LLM available: {adapter.is_available()}")
    
    if adapter.is_available():
        # Test incident
        incident = {
            "service": "api_gateway",
            "status": {"running": False, "healthy": False},
            "metrics": {"cpu_percent": 0, "memory_mb": 0, "is_failed": True, "failure_type": "crash"}
        }
        
        plan = adapter.generate_plan(incident)
        if plan:
            print("Generated LLM plan:")
            for step in plan:
                print(f"  {step['step']} ({step['agent']}) - {step.get('description', '')}")
        else:
            print("Failed to generate LLM plan")
    else:
        print("LLM not available, would fall back to stub planner")


if __name__ == "__main__":
    import time
    main()

