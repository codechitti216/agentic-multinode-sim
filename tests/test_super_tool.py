# Auto-generated by Manus: Chaos Triage Arena scaffold

import pytest
import time
import tempfile
import json
from pathlib import Path
from unittest.mock import Mock, patch

# Import SuperTool
import sys
sys.path.append(str(Path(__file__).parent.parent))

from super_tool import SuperTool


@pytest.fixture
def temp_arena_config():
    """Create temporary arena config for testing"""
    config = {
        "services": {
            "test_service": {
                "port": 9001,
                "script": "test_service.py",
                "dependencies": []
            },
            "dependent_service": {
                "port": 9002,
                "script": "dependent_service.py",
                "dependencies": ["test_service"]
            }
        }
    }
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(config, f)
        config_path = f.name
    
    yield config_path
    
    # Cleanup
    Path(config_path).unlink()


@pytest.fixture
def super_tool(temp_arena_config):
    """Create SuperTool instance for testing"""
    return SuperTool(temp_arena_config)


def test_super_tool_initialization(super_tool):
    """Test SuperTool initialization"""
    assert super_tool.arena_config is not None
    assert "services" in super_tool.arena_config
    assert "test_service" in super_tool.arena_config["services"]
    assert len(super_tool._action_history) == 0


def test_record_action_event(super_tool):
    """Test recording action events"""
    # Record an action
    super_tool.record_action_event("test_service", "start", 1.5, {"ok": True})
    
    # Check it was recorded
    history = super_tool.get_action_history()
    assert len(history) == 1
    
    action = history[0]
    assert action["service"] == "test_service"
    assert action["action"] == "start"
    assert action["elapsed"] == 1.5
    assert action["result"]["ok"] is True
    assert "timestamp" in action


def test_action_history_limit(super_tool):
    """Test action history size limit"""
    # Add more than the limit (1000)
    for i in range(1100):
        super_tool.record_action_event(f"service_{i % 5}", "test", 0.1, {"ok": True})
    
    # Should be limited to 1000
    history = super_tool.get_action_history()
    assert len(history) == 1000


def test_get_action_metrics(super_tool):
    """Test action metrics calculation"""
    # Add some test actions
    super_tool.record_action_event("test_service", "start", 1.0, {"ok": True})
    super_tool.record_action_event("test_service", "stop", 0.5, {"ok": True})
    super_tool.record_action_event("test_service", "start", 2.0, {"ok": True})
    
    metrics = super_tool.get_action_metrics()
    
    assert "test_service" in metrics
    service_metrics = metrics["test_service"]
    
    assert service_metrics["count"] == 3
    assert service_metrics["avg_duration"] == 1.5  # (1.0 + 0.5 + 2.0) / 3
    assert service_metrics["max_duration"] == 2.0
    assert service_metrics["min_duration"] == 0.5


def test_get_action_metrics_empty(super_tool):
    """Test action metrics with no actions"""
    metrics = super_tool.get_action_metrics()
    
    # Should have entries for all services, even with no actions
    assert "test_service" in metrics
    service_metrics = metrics["test_service"]
    
    assert service_metrics["count"] == 0
    assert service_metrics["avg_duration"] == 0
    assert service_metrics["max_duration"] == 0
    assert service_metrics["min_duration"] == 0


@patch('super_tool.requests.get')
def test_verify_service_health_success(mock_get, super_tool):
    """Test successful health verification"""
    # Mock successful health check
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"status": "healthy"}
    mock_get.return_value = mock_response
    
    result = super_tool._verify_service_health("test_service")
    assert result is True
    assert "Healthy via /healthz" in super_tool._health_last_reason["test_service"]


@patch('super_tool.requests.get')
def test_verify_service_health_fallback(mock_get, super_tool):
    """Test health verification fallback to /status"""
    # Mock /healthz failure, /status success
    def side_effect(url, **kwargs):
        if "/healthz" in url:
            raise Exception("Connection failed")
        elif "/status" in url:
            mock_response = Mock()
            mock_response.status_code = 200
            return mock_response
    
    mock_get.side_effect = side_effect
    
    result = super_tool._verify_service_health("test_service")
    assert result is True
    assert "Healthy via /status" in super_tool._health_last_reason["test_service"]


@patch('super_tool.requests.get')
def test_verify_service_health_failure(mock_get, super_tool):
    """Test health verification failure"""
    # Mock both endpoints failing
    mock_get.side_effect = Exception("Connection refused")
    
    result = super_tool._verify_service_health("test_service")
    assert result is False
    assert "Health check failed" in super_tool._health_last_reason["test_service"]


def test_verify_service_health_unknown_service(super_tool):
    """Test health verification for unknown service"""
    result = super_tool._verify_service_health("unknown_service")
    assert result is False
    assert "Service not in config" in super_tool._health_last_reason["unknown_service"]


@patch('super_tool.subprocess.Popen')
@patch('super_tool.SuperTool._verify_service_health')
def test_start_service_success(mock_health, mock_popen, super_tool):
    """Test successful service start"""
    # Mock process and health check
    mock_process = Mock()
    mock_process.pid = 12345
    mock_process.poll.return_value = None  # Still running
    mock_popen.return_value = mock_process
    mock_health.return_value = True
    
    # Create mock script file
    script_path = super_tool.project_root / "test_service.py"
    script_path.touch()
    
    try:
        result = super_tool.start_service("test_service")
        
        assert result["ok"] is True
        assert result["pid"] == 12345
        assert result["port"] == 9001
        
        # Should be in processes dict
        assert "test_service" in super_tool._processes
        
    finally:
        # Cleanup
        if script_path.exists():
            script_path.unlink()


def test_start_service_unknown(super_tool):
    """Test starting unknown service"""
    result = super_tool.start_service("unknown_service")
    
    assert result["ok"] is False
    assert "Service not found" in result["error"]


def test_start_service_missing_script(super_tool):
    """Test starting service with missing script"""
    result = super_tool.start_service("test_service")
    
    assert result["ok"] is False
    assert "Service script not found" in result["error"]


def test_stop_service_not_running(super_tool):
    """Test stopping service that's not running"""
    result = super_tool.stop_service("test_service")
    
    assert result["ok"] is True
    assert "Service not running" in result["message"]


def test_kill_service_not_running(super_tool):
    """Test killing service that's not running"""
    result = super_tool.kill_service("test_service")
    
    assert result["ok"] is True
    assert "Service not running" in result["message"]


@patch('super_tool.requests.post')
def test_inject_failure_success(mock_post, super_tool):
    """Test successful failure injection"""
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"ok": True}
    mock_post.return_value = mock_response
    
    result = super_tool.inject_failure("test_service", "database_error")
    
    assert result["ok"] is True
    assert result["via"] == "service"


@patch('super_tool.requests.post')
def test_inject_failure_fallback(mock_post, super_tool):
    """Test failure injection fallback to simulation"""
    mock_post.side_effect = Exception("Connection failed")
    
    result = super_tool.inject_failure("test_service", "database_error")
    
    assert result["ok"] is True
    assert result["via"] == "simulated"
    assert "test_service" in super_tool._sim_state
    assert super_tool._sim_state["test_service"]["failure_database_error"] is True


def test_inject_failure_unknown_service(super_tool):
    """Test failure injection for unknown service"""
    result = super_tool.inject_failure("unknown_service", "error")
    
    assert result["ok"] is False
    assert "Service not found" in result["error"]


@patch('super_tool.requests.post')
def test_cpu_stress_success(mock_post, super_tool):
    """Test successful CPU stress"""
    mock_response = Mock()
    mock_response.status_code = 200
    mock_post.return_value = mock_response
    
    result = super_tool.cpu_stress("test_service", 70)
    
    assert result["ok"] is True
    assert result["via"] == "service"


@patch('super_tool.requests.post')
def test_cpu_stress_fallback(mock_post, super_tool):
    """Test CPU stress fallback to simulation"""
    mock_post.side_effect = Exception("Connection failed")
    
    result = super_tool.cpu_stress("test_service", 70)
    
    assert result["ok"] is True
    assert result["via"] == "simulated"
    assert super_tool._sim_state["test_service"]["cpu_stress"] == 70


@patch('super_tool.requests.get')
def test_get_all_metrics_success(mock_get, super_tool):
    """Test successful metrics collection"""
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        "cpu_percent": 25.5,
        "memory_mb": 128.0,
        "timestamp": time.time()
    }
    mock_get.return_value = mock_response
    
    metrics = super_tool.get_all_metrics()
    
    assert "test_service" in metrics
    assert "dependent_service" in metrics
    assert metrics["test_service"]["cpu_percent"] == 25.5


@patch('super_tool.requests.get')
def test_get_all_metrics_with_simulation(mock_get, super_tool):
    """Test metrics collection with simulation state"""
    mock_get.side_effect = Exception("Connection failed")
    
    # Add some simulation state
    super_tool._sim_state["test_service"] = {"cpu_stress": 80}
    
    metrics = super_tool.get_all_metrics()
    
    assert "test_service" in metrics
    assert metrics["test_service"]["error"] == "unreachable"
    assert metrics["test_service"]["simulated"]["cpu_stress"] == 80


if __name__ == "__main__":
    pytest.main([__file__, "-v"])

